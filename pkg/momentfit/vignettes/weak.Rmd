---
title: "Instrumental Variables Tools for the Case of Weak or Many Instruments"
author: "Pierre Chausse^[University of Waterloo, pchausse@uwaterloo.ca]"
date: ""
bibliography: empir.bib 
output:
 pdf_document:
  number_sections: true
abstract: "This vignette explains the different tools included in the package to deal with the weak or the many instruments problem. For example, it presents estimation methods like the LIML or its modified version proposed by @fuller77 method and some improved inference methods for TSLS and GMM. It is in early stage of development, so comments and recommendations are welcomed."
vignette: >
  %\VignetteIndexEntry{Instrumental Variables Tools for the Case of Weak or Many Instruments}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteDepends{momentfit}
  %\VignettePackage{momentfit}
  %\VignetteEncoding{UTF-8}
header-includes:
- \newcommand{\E}{\mathrm{E}}
- \newcommand{\diag}{\mathrm{diag}}
- \newcommand{\Prob}{\mathrm{Pr}}
- \newcommand{\Var}{\mathrm{Var}}
- \newcommand{\Vect}{\mathrm{Vec}}
- \newcommand{\Cov}{\mathrm{Cov}}
- \newcommand{\Cor}{\mathrm{Cor}}
- \newcommand{\conP}{\overset{p}{\to}}
- \newcommand{\conD}{\overset{d}{\to}}
- \newcommand\R{ \mathbb{R} }
- \newcommand\N{ \mathbb{N} }
- \newcommand\C{ \mathbb{C} }
- \newcommand\rv{{\cal R}}
- \newcommand\Q{\mathbb{Q}}
- \newcommand\PR{{\cal R}}
- \newcommand\T{{\cal T}}
- \newcommand\Hi{{\cal H}}
- \newcommand\La{{\cal L}}
- \newcommand\plim{plim}
- \renewcommand{\epsilon}{\varepsilon}
---

```{r, echo=FALSE}
library(knitr)
opts_chunk$set(size='footnotesize')
```

**Important**: This document is incomplete (so is the package for what is covered
here).

# The model

We only consider linear models for the moment. Let the following be
the model of interest:

\[
y = X_1\beta_1 + X_2\beta_2+u \equiv X\beta + u\,
\]

where $y$ and $u$ are $n\times 1$, $X_1$ is $n\times k_1$, $X_2$ is
$n\times k_2$, $\beta_1$ is $k_1\times 1$, $\beta_2$ is $k_2\times 1$,
$X$ is $n \times k$ and $\beta$ is $k\times 1$, with $k=k_1+k_2$. We
assume that the intercept is included in $X_1$.  Suppose that $X_2$ is
the matrix of endogenous variables. Then, we want to instrument them
with $Z_2$, a $n\times l_2$ matrix, where $l_2\geq k_2$. The matrix of
exogenous variables that are included and excluded is $Z=[X_1,
Z_2]$, a $n\times q$ matrix with $q=k_1+l_2$. The reduced form for
$X_2$, or the first stage regression, is therefore:

\[
X_2 = Z\Pi + e\,,
\]

where $\Pi$ is $q\times k_2$ and $e$ is $n\times k_2$. 


# K-class Estimator and LIML

The K-Class methods need to be added to the package if we want to
develop tools for models with weak and/or many instruments. The reason
is that estimations and tests based on the limited information maximum
likelihood (LIML), which is K-Class method, has shown to perform well
in these cases. 

To my knowledge, many of the methods proposed here have not been
implemented in R yet. However, some procedures are implemented in the
`ivmodel` package of @ivmodel. Some of our procedures have been
influenced by the package, so we use it when needed to compare our
results.

## The method

A K-Class estimator is the solution to 

\[
X'(I-\kappa M_z)(y-X\beta)=0\,,
\]

where $M_z=I-P_z$ and $P_z$ is the projection matrix $Z(Z'Z)^{-1}Z'$. It
is therefore represented as a just-identified IV with the instrument
$W_\kappa=(I-\kappa M_z)X$. Note that $M_zX_1=0$, which implies the following matrix of
instruments:

\[
\begin{split}
W_\kappa & = \begin{bmatrix}
(I-\kappa M_z)X_1 & (I-\kappa M_z)X_2
\end{bmatrix} \\ & = 
\begin{bmatrix}
X_1 & (I-\kappa M_z)X_2
\end{bmatrix}\\
& = 
\begin{bmatrix}
X_1 & (X_2-\kappa\hat{e})
\end{bmatrix}
\end{split}\,,
\]

where $\hat{e}=M_zX_2$ is the matrix of residuals from the first stage
regression. Note that the model is just-identified only when
$l_2>k_2$. The above representation is just a convenient way of
defining the method. In fact, we can also represent the two-stage
least squares (TSLS) method , over-identified or not, as a
just-identified IV with $W=[X_1\hat{X}_2]$, where
$\hat{X}_2=P_zX_2\equiv X_2-\hat{e}$. Therefore, TSLS is a K-Class
estimator with $\kappa=1$. We can also see that the least squares estimator
can be obtained by setting $\kappa$ to 0. The solution can be written as
follows:

\[
\hat{\beta}_\kappa = (W_\kappa'X)^{-1}W_\kappa'y\,.
\]

We can compute the standard errors using the asymptotic
properties of just identified IV. In the case of iid errors (no
heteroskedasticity), the variance can be estimated as:

\[
\hat\Sigma_{\kappa,iid} = 
\hat{\sigma}^2(W_\kappa'X)^{-1} W_\kappa'W_\kappa (W_\kappa'X)^{-1}\,,
\]

where $\hat{\sigma}^2$ is the estimated variance of $u$. Note that the
bread of the covariance matrix is symmetric, which is not the case in
general for just-identified IV. Also, we can simplify the expression
to $\hat{\sigma}^2(W_\kappa'X)^{-1}$ only when $\kappa$ is equal to 0
or 1. For other values it is not possible because $(I-\kappa
M_z)(I-\kappa M_z)\neq (I-\kappa M_z)$. In the case of heteroskedastic
errors, the covariance matrix can be estimated as follows:

\[
\hat\Sigma_{\kappa,HC} = (W_\kappa'X)^{-1} \hat\Omega_{\kappa,HC} (W_\kappa'X)^{-1}\,,
\]

where $\hat\Omega_{HC}$ is an HCCM estimator of the variance of
$W_\kappa'u$. For example, we can obtain the HC0 estimator with the
following $\hat\Omega$:

\[
\hat\Omega_{\kappa,HC0} = \sum_{i=1}^n \hat{u}_i^2 W_{\kappa, i}W_{\kappa, i}'\,,
\]

where $\hat{u}_i = y_i-X_i'\hat{\beta}_\kappa$.


## The LIML method

We do not justify how $\kappa$ is defined for the LIML method. For
more details, see @davidson-mackinnon04. Let $Y=[y~ X_2]$ be the
$n\times (1+k_2)$ matrix with all endogenous variables from the
model. Then, $\kappa_{liml}$ is defined as the smallest eigenvalue of:

\[
(Y'M_zY)^{-1/2}Y'M_1Y(Y'M_zY)^{-1/2}\,,
\]

where $M_1=I-P_1$ and $P_1=X_1(X_1'X_1)^{-1}X_1'$. We can show that it
is equivalent to finding the smallest eigenvalue of
$(Y'M_zY)^{-1}Y'M_1Y$. An alternative to the LIML method was proposed
by @fuller77. The method is also a K-Class method with
$\kappa_{ful}=\kappa_{liml}-\alpha/(n-q)$, where $\alpha$ is
parameter. It usually set to=1. The Fuller method happens to have
better properties than LIML.

## Computing $\hat\kappa$

We want to use the data used by Card (1993). The dataset is included
in the `ivmodel` package. The endogenous variable is education
(`educ`) and the two instruments we consider are `near4` and
`near2`. The other included exogenous variables are experience
(`exper`), experience squared (`expersq`) and a set of binary
variables. In the following, the `ivmodel` object is generate. It
contains the `\kappa` for LIML and Fuller:

```{r}
library(ivmodel)
data(card.data)
## from the ivmodel examples
Z <- card.data[,c("nearc4","nearc2")]
Y <- card.data[,"lwage"]
D <- card.data[,"educ"]
Xname <- c("exper", "expersq", "black", "south", "smsa", "reg661",
           "reg662", "reg663", "reg664", "reg665", "reg666", "reg667",
           "reg668", "smsa66")
X <- card.data[,Xname]
mod <- ivmodel(Y=Y,D=D,Z=Z,X=X)
```

We can see the $\kappa$'s using the following commands:

```{r}
c(LIML=mod$LIML$k, Fuller=mod$Fuller$k)
```

We can create a `linearModel` object with the same specifications as
follows. By default, `ivmodel` model assumes homoskedasticity, so we
set the argument `vcov` to `"iid"`:

```{r}
library(momentfit)
g <- reformulate(c("educ", Xname), "lwage")
h <- reformulate(c(c("nearc4","nearc2"), Xname))
mod2 <- momentModel(g, h, data=card.data, vcov="iid")
```

The `getK` function generates $\hat\kappa$ for the original LIML and
the modified one. No effort is done to make it efficient for now. The
modified LIML is $\hat\kappa - \alpha/(n-k)$, where $k$ is the number
of exogenous variables (included and excluded).

We can compare the values with the ones computed by `ivmodel`. They
are identical:

```{r}
getK(mod2)
```

Note that the function `getK` has three arguments: `object`, which is
the model object, `alpha`, which is use to compute $\kappa_{ful}$ and
`returnRes`. When the latter is set to `TRUE` (the default is
`FALSE`), the function returns a list of two elements: the above
vector of $\kappa$ and the matrix of first stage residuals
$M_zX_2$. The latter is used by the K-Class function to generate the
matrix of instruments $W_\kappa$. By setting it to `TRUE`, it avoids
having to recompute it.

We can also have more than one endogenous regressor. For this model,
we can interact `educ` with, say, `exper`, which is like having a
second endogenous variable. The package can recognize that
`educ:exper` is endogenous because it is not part of the set of
instruments. The following is the new model:

```{r}
g2 <- reformulate(c("educ", "educ:exper", Xname), "lwage")
h2 <- reformulate(c(c("nearc4","nearc2", "nearc2:exper", "nearc4:exper"), Xname))
mod3 <- momentModel(g2, h2, data=card.data)
getK(mod3)
```

Note that $\kappa_{liml}=1$ for just-identified models. When it is the
case, `getK` does not compute the residuals and only returns the
vector of $\kappa$ no matter how we set the argument `returnRes`. The
following model is just identified:

```{r}
h3 <- reformulate(c(c("nearc4"), Xname))
mod4 <- momentModel(g, h3, data=card.data)
getK(mod4)
```

## Computing the K-Class estimators

The function that computes the K-Class estimator is `kclassfit`. The
arguments are: `object`, the model object, `k`, the value of $\kappa$,
`type`, the type of $\kappa$ to compute when `k` is missing (`"LIML"`,
`"Fuller"` or `"BTSLS"` for the biased corrected TSLE) and `alpha`,
the parameter of the Fuller method (the default is 1). Note first that
the estimator is a TSLS estimator when `k=1` and a LSE when it is
equal to 0. The package already has a `tsls` method for `linearModel`
objects, which is what `kclassfit` calls when `k=1`. For the LSE, a
new method was created to facilitate the estimation of model objects
by least squares. The method is `lse`:

```{r}
lse(mod2)
```

It is an object of class `lsefit` that contains the `lm` object from
the estimation. Therefore, the `kclassfit` function returns an object
of class `lsefit` when `k=0` and `tlsl` when `k=1`. For any other
value, which includes LIML, Fuller and BTSLS ($\kappa=n/(n-l_2+2)$),
the function returns an object of class `kclassfit`. The object
contains a `gmmfit` object, generated by the estimation of the
artificially created just-identified model, the name of the method,
the value of $\kappa$ and the original model.

```{r}
(liml <- kclassfit(mod2))
(fuller <- kclassfit(mod2, type="Fuller"))
(btsls <- kclassfit(mod2, type="BTSLS"))
```

Note that the biased-adjusted TSLS is just TSLS because the adjustment
only affects the method when the number of excluded instruments is not
equal to 2. We see in the following that the LIML and Fuller estimates
I get are identical to the ones from the `ivmodel` package.

```{r}
print(mod$LIML$point.est,digits=10)
print(coef(liml)[2], digits=10)
```

```{r}
print(mod$Fuller$point.est,digits=10)
print(coef(fuller)[2], digits=10)
```

Note that the argument `k` can be the output of `getK` with
`returnRes=TRUE`. This is a way of avoiding recomputing the $\kappa$
and the first stage residuals. This is useful when we want to compute
the LIML and Fuller for the same model. For example, the following is
the fast version of what we did above.

```{r}
resK <- getK(mod2, 1, TRUE)
(liml <- kclassfit(mod2, resK))
(fuller <- kclassfit(mod2, resK, type="Fuller"))
```

## Inference

Since the `kclassfit` object contains a just-identified `gmmfit`
object, we can do inference as if it was an IV. The `summary` method
for `kclassfit` objects is in fact the same as for `gmmfit` objects,
but it contains additional information about the original model and
the method. It returns an object of class `summaryKclass`.

```{r}
(s <- summary(liml))
```

Note that the specification test is based on Anderson and Rubin. It is
a likelihood ratio test equal to $n\log(\hat\kappa)$ and is
distributed as a chi-square with the degrees of freedom equal to the
number of over-identifying restrictions. It calls the `specTest`
method for `kclassfit` objects:

```{r}
specTest(liml)
```

We can compare the standard error we get here and the one we get from
the `ivmodel` package. Note that only inference about the coefficient
of the endogenous variable is provided by `ivmodel`.

```{r}
s@coef["educ",]
mod$LIML$std.err
```

The result is quite different. But we can see why. In the following I
recompute the standard error using the formula
$\hat{\sigma}^2(W_\kappa'X)^{-1}$. We now get the same result. As mentioned
before, this expression is only valid for $\kappa=1$.

```{r}
spec <- modelDims(mod2)
u <- residuals(liml)
sig <- sum(u^2)/(spec$n-spec$k)
W <- model.matrix(liml@model, "instruments")
myX <- model.matrix(liml@model)
sqrt(diag(sig*solve(t(W)%*%myX)))[2]
```

For Heteroskedastic errors. We have to redefine the models. 

```{r, eval=FALSE}
mod <- ivmodel(Y=Y,D=D,Z=Z,X=X,heteroSE=TRUE)
mod2 <- momentModel(g, h, data=card.data, vcov="MDS")
liml <- kclassfit(mod2, resK)
summary(liml)@coef["educ",]
c(mod$LIML$point.est, mod$LIML$std.err)
```

The above code is not run because the `ivmodel` is very inefficient to
compute the meat matrix. It is done using a loop. It you run the code
you should get identical point estimate and both standard errors are
equal to 0.0576098.

# Weak Instruments 

## Testing for weak instrument: Stock-Yogo (2005)

This test and the critical values are for model with homoskedastic
errors. The test is the smallest eigenvalue of the following:

\[
\hat\Sigma_e^{-1/2}
\Big[
X_2'M_1Z_2(Z_2'M_1Z_2)^{-1}Z_2'M_1X_2 
\Big]
\hat\Sigma_e^{-1/2} = [M_1 X_2]'[Z_2\hat\Pi_2]
\]

where $\hat\Sigma_e = \hat{e}'\hat{e}/(n-l_2-k_1)$. The function
`CDtest`, which stands for Cragg and Donald test, computes this
statistic. We can see that it returns the F-statistics when the number
of endogenous variables is equal to 1:

```{r}
CDtest(mod2, print=FALSE)
momentStrength(mod2)
```

Here is what we obtain for `mod3`

```{r}
CDtest(mod3)
```

```{r}
g <- reformulate(c("educ", Xname), "lwage")
h <- reformulate(c(c("nearc4","nearc2","IQ","KWW"), Xname))
mod5 <- momentModel(g, h, data=card.data, vcov="iid")
CDtest(mod5)
```




## Data Generating Process (for later use)

The following function is used to generate dataset with $k$
instruments and different level of strength. The DGP is

\[
y_1 = \beta y_2 + u
\]
\[
y_2 = \pi'Z + e\,,
\]

where $Z\in\R^k$, $\Var(u)=\Var(e)=1$, $\Cor(e,u)=\rho$, $\pi_i=\eta$
for all $i=1,...,k$ and $Z\sim N(0,I)$. The $R^2$ of the first stage
regression is therefore equal to

\[
R^2 = \frac{k\eta^2}{k\eta^2+1}\,,
\]

which implies

\[
\eta = \sqrt{\frac{R^2}{k(1-R^2)}}
\]

We can therefore set $R^2$ and $k$ and let the function get $\eta$. 

```{r}
getIVDat <- function(n, R2, k, rho, b0=0)
{
    eta <- sqrt(R2/(k*(1-R2)))
    Z <- sapply(1:k, function(i) rnorm(n))
    sigma <- chol(matrix(c(1,rho,rho,1),2,2))
    err <- cbind(rnorm(n), rnorm(n))%*%sigma
    y2 <- rowSums(Z)*eta+err[,2]
    y1 <- b0*y2 + err[,1]
    dat <- data.frame(y1=y1, y2=y2, u=err[,1], e=err[,2])
    for (i in 1:k) dat[[paste("Z",i,sep="")]] <- Z[,i]
    dat
}
```

```{r}
library(momentfit)
set.seed(112233)
k <- 10
rho <- .3
R2 <- .001
g <- y1~y2
n <- 500
h <- reformulate(paste("Z", 1:k, sep=""))
dat <- getIVDat(n, R2, k, rho)
m <- momentModel(g, h, data=dat, vcov="MDS")
``` 



# References


